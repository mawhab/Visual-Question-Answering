{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install opendatasets","metadata":{"id":"riIg0bj8IbLf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import opendatasets as od\n\nod.download('https://www.kaggle.com/datasets/ingbiodanielh/vizwiz')","metadata":{"id":"eFhvv7zcI-st"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls /content/vizwiz/data/Images | wc -l","metadata":{"id":"dKO3pQ5guhhn","outputId":"cb3ab005-a79f-4d19-e150-49444634a0da"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install ftfy regex tqdm\n!pip install git+https://github.com/openai/CLIP.git","metadata":{"id":"2gPqaWaRJJSv","_kg_hide-output":false,"_kg_hide-input":false,"execution":{"iopub.status.busy":"2023-06-19T23:23:48.026242Z","iopub.execute_input":"2023-06-19T23:23:48.026510Z","iopub.status.idle":"2023-06-19T23:24:17.413075Z","shell.execute_reply.started":"2023-06-19T23:23:48.026486Z","shell.execute_reply":"2023-06-19T23:24:17.411764Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting ftfy\n  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (2023.5.5)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.64.1)\nRequirement already satisfied: wcwidth>=0.2.5 in /opt/conda/lib/python3.10/site-packages (from ftfy) (0.2.6)\nInstalling collected packages: ftfy\nSuccessfully installed ftfy-6.1.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting git+https://github.com/openai/CLIP.git\n  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-0rrjo_x5\n  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-0rrjo_x5\n  Resolved https://github.com/openai/CLIP.git to commit a9b1bf5920416aaeaec965c25dd9e8f98c864f16\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: ftfy in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (6.1.1)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2023.5.5)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (4.64.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2.0.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (0.15.1)\nRequirement already satisfied: wcwidth>=0.2.5 in /opt/conda/lib/python3.10/site-packages (from ftfy->clip==1.0) (0.2.6)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.12.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (1.23.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (2.28.2)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (9.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->clip==1.0) (2.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (2023.5.7)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->clip==1.0) (1.3.0)\nBuilding wheels for collected packages: clip\n  Building wheel for clip (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369410 sha256=a317cc0e15a20e0805963f7e3cb8b59264ebb4079c3ed37ac02b9541fd21b833\n  Stored in directory: /tmp/pip-ephem-wheel-cache-w8_h42qs/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\nSuccessfully built clip\nInstalling collected packages: clip\nSuccessfully installed clip-1.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import clip\nimport json\nimport numpy as np\nimport torch\nimport torchtext\nfrom PIL import Image\nfrom IPython.display import Image as IM\nfrom IPython.display import display\nimport torch.nn as nn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"id":"vQQPrvxML-cr","execution":{"iopub.status.busy":"2023-06-19T23:24:17.415464Z","iopub.execute_input":"2023-06-19T23:24:17.415877Z","iopub.status.idle":"2023-06-19T23:24:22.549024Z","shell.execute_reply.started":"2023-06-19T23:24:17.415837Z","shell.execute_reply":"2023-06-19T23:24:22.548083Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"model, preprocess = clip.load('ViT-B/32', device=device)","metadata":{"id":"7M3L5vFZa641","outputId":"8e35a7b2-8a68-434a-eb24-fcf2f7246fdf","execution":{"iopub.status.busy":"2023-06-20T00:33:59.421754Z","iopub.execute_input":"2023-06-20T00:33:59.422160Z","iopub.status.idle":"2023-06-20T00:34:03.812023Z","shell.execute_reply.started":"2023-06-20T00:33:59.422131Z","shell.execute_reply":"2023-06-20T00:34:03.811056Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"img = Image.open('/content/vizwiz/data/Images/VizWiz_train_000000000000.jpg')","metadata":{"id":"x7Bt3X8ZZ6XG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img.show()","metadata":{"id":"VKATnamsaG_6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(IM(filename='/kaggle/input/vizwiz/data/Images/VizWiz_train_000000000001.jpg'))","metadata":{"execution":{"iopub.status.busy":"2023-06-19T00:37:26.957468Z","iopub.execute_input":"2023-06-19T00:37:26.957840Z","iopub.status.idle":"2023-06-19T00:37:26.973905Z","shell.execute_reply.started":"2023-06-19T00:37:26.957809Z","shell.execute_reply":"2023-06-19T00:37:26.972925Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = None\n\nwith open('/kaggle/input/vizwiz/data/Annotations/train.json') as f:\n  data = json.load(f)","metadata":{"id":"OOz9mpcFITJr","execution":{"iopub.status.busy":"2023-06-19T23:24:32.830681Z","iopub.execute_input":"2023-06-19T23:24:32.831050Z","iopub.status.idle":"2023-06-19T23:24:33.196564Z","shell.execute_reply.started":"2023-06-19T23:24:32.831017Z","shell.execute_reply":"2023-06-19T23:24:33.195215Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/input/vizwiz/data/Annotations/val.json') as f:\n  val_data = json.load(f)","metadata":{"execution":{"iopub.status.busy":"2023-06-20T00:33:34.359435Z","iopub.execute_input":"2023-06-20T00:33:34.359858Z","iopub.status.idle":"2023-06-20T00:33:34.427781Z","shell.execute_reply.started":"2023-06-20T00:33:34.359798Z","shell.execute_reply":"2023-06-20T00:33:34.426824Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"def get_answers_from_dict(answers_dict):\n    answers = [entry['answer'] for entry in answers_dict]\n    return answers","metadata":{"execution":{"iopub.status.busy":"2023-06-19T23:24:33.198054Z","iopub.execute_input":"2023-06-19T23:24:33.198419Z","iopub.status.idle":"2023-06-19T23:24:33.205252Z","shell.execute_reply.started":"2023-06-19T23:24:33.198383Z","shell.execute_reply":"2023-06-19T23:24:33.202543Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def get_answer(model, preprocess, img_path, answers):\n    image = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n    text = clip.tokenize(answers).to(device)\n\n    with torch.no_grad():\n        image_features = model.encode_image(image)\n        text_features = model.encode_text(text)\n\n        logits_per_image, logits_per_text = model(image, text)\n        probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n\n    return np.argmax(probs)","metadata":{"execution":{"iopub.status.busy":"2023-06-19T23:24:33.210322Z","iopub.execute_input":"2023-06-19T23:24:33.211341Z","iopub.status.idle":"2023-06-19T23:24:33.258099Z","shell.execute_reply.started":"2023-06-19T23:24:33.211206Z","shell.execute_reply":"2023-06-19T23:24:33.255432Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"get_answer(model, preprocess, '/kaggle/input/vizwiz/data/Images/VizWiz_train_000000000001.jpg', get_answers_from_dict(data[1]['answers']))","metadata":{"execution":{"iopub.status.busy":"2023-06-19T00:39:28.382572Z","iopub.execute_input":"2023-06-19T00:39:28.383183Z","iopub.status.idle":"2023-06-19T00:39:28.468724Z","shell.execute_reply.started":"2023-06-19T00:39:28.383152Z","shell.execute_reply":"2023-06-19T00:39:28.467820Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_answers_from_dict(data[1]['answers'])","metadata":{"execution":{"iopub.status.busy":"2023-06-19T00:37:01.490572Z","iopub.execute_input":"2023-06-19T00:37:01.490964Z","iopub.status.idle":"2023-06-19T00:37:01.498358Z","shell.execute_reply.started":"2023-06-19T00:37:01.490908Z","shell.execute_reply":"2023-06-19T00:37:01.497415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = []\n\nfor i,entry in enumerate(data):\n    idx = get_answer(model, preprocess, '/kaggle/input/vizwiz/data/Images/'+entry['image'], get_answers_from_dict(entry['answers']))\n    df.append([entry['image'], entry['answerable'], entry['question'], entry['answer_type'], entry['answers'][idx]['answer']])\n    if i%1000==0:\n        print('Checkpoint at: ' + str(i))\n        \ndf = np.array(df)","metadata":{"id":"7BOZkF7bKLOV","execution":{"iopub.status.busy":"2023-06-19T02:21:51.954951Z","iopub.execute_input":"2023-06-19T02:21:51.955286Z","iopub.status.idle":"2023-06-19T10:51:22.087541Z","shell.execute_reply.started":"2023-06-19T02:21:51.955258Z","shell.execute_reply":"2023-06-19T10:51:22.084203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/working/df.npy', 'wb') as f:\n    np.save(f,df)","metadata":{"execution":{"iopub.status.busy":"2023-06-19T10:51:22.092547Z","iopub.execute_input":"2023-06-19T10:51:22.093120Z","iopub.status.idle":"2023-06-19T10:51:22.260052Z","shell.execute_reply.started":"2023-06-19T10:51:22.093062Z","shell.execute_reply":"2023-06-19T10:51:22.258818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/input/dfasdasd/df.npy', 'rb') as f: \n    df = np.load(f)","metadata":{"execution":{"iopub.status.busy":"2023-06-19T23:24:33.260715Z","iopub.execute_input":"2023-06-19T23:24:33.261061Z","iopub.status.idle":"2023-06-19T23:24:33.943026Z","shell.execute_reply.started":"2023-06-19T23:24:33.261029Z","shell.execute_reply":"2023-06-19T23:24:33.942039Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"answers = np.array([entry[4] for entry in df])","metadata":{"id":"bbEHlXU8f58g","execution":{"iopub.status.busy":"2023-06-19T23:24:33.944555Z","iopub.execute_input":"2023-06-19T23:24:33.944996Z","iopub.status.idle":"2023-06-19T23:24:34.006498Z","shell.execute_reply.started":"2023-06-19T23:24:33.944954Z","shell.execute_reply":"2023-06-19T23:24:34.005580Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2023-06-19T23:24:34.007823Z","iopub.execute_input":"2023-06-19T23:24:34.008163Z","iopub.status.idle":"2023-06-19T23:24:35.019458Z","shell.execute_reply.started":"2023-06-19T23:24:34.008130Z","shell.execute_reply":"2023-06-19T23:24:35.018333Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"(20000, 5)"},"metadata":{}}]},{"cell_type":"code","source":"answer_types = np.array([entry[3] for entry in df])","metadata":{"id":"1fe_Q2_wf-7L","execution":{"iopub.status.busy":"2023-06-19T23:24:35.117516Z","iopub.execute_input":"2023-06-19T23:24:35.117868Z","iopub.status.idle":"2023-06-19T23:24:35.170914Z","shell.execute_reply.started":"2023-06-19T23:24:35.117836Z","shell.execute_reply":"2023-06-19T23:24:35.170063Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"val_df = []\n\nfor i,entry in enumerate(val_data):\n    idx = get_answer(model, preprocess, '/kaggle/input/vizwiz/data/Images/'+entry['image'], get_answers_from_dict(entry['answers']))\n    val_df.append([entry['image'], entry['answerable'], entry['question'], entry['answer_type'], entry['answers'][idx]['answer']])\n    if i%1000==0:\n        print('Checkpoint at: ' + str(i))\n        \nval_df = np.array(val_df)","metadata":{"execution":{"iopub.status.busy":"2023-06-20T00:34:09.163494Z","iopub.execute_input":"2023-06-20T00:34:09.163897Z","iopub.status.idle":"2023-06-20T00:39:10.523339Z","shell.execute_reply.started":"2023-06-20T00:34:09.163864Z","shell.execute_reply":"2023-06-20T00:39:10.522362Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"Checkpoint at: 0\nCheckpoint at: 1000\nCheckpoint at: 2000\nCheckpoint at: 3000\n","output_type":"stream"}]},{"cell_type":"code","source":"with open('/kaggle/working/val_df1.npy', 'wb') as f:\n    np.save(f,val_df)","metadata":{"execution":{"iopub.status.busy":"2023-06-20T00:46:00.025335Z","iopub.execute_input":"2023-06-20T00:46:00.025710Z","iopub.status.idle":"2023-06-20T00:46:00.044808Z","shell.execute_reply.started":"2023-06-20T00:46:00.025679Z","shell.execute_reply":"2023-06-20T00:46:00.043855Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/input/dfasdasd/val_df.npy', 'rb') as f: \n    val_df = np.load(f)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_answers = np.array([entry[4] for entry in val_df])","metadata":{"execution":{"iopub.status.busy":"2023-06-20T00:40:50.420714Z","iopub.execute_input":"2023-06-20T00:40:50.421098Z","iopub.status.idle":"2023-06-20T00:40:50.434468Z","shell.execute_reply.started":"2023-06-20T00:40:50.421070Z","shell.execute_reply":"2023-06-20T00:40:50.433527Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"val_answer_types = np.array([entry[3] for entry in val_df])","metadata":{"execution":{"iopub.status.busy":"2023-06-20T00:40:52.305881Z","iopub.execute_input":"2023-06-20T00:40:52.306242Z","iopub.status.idle":"2023-06-20T00:40:52.319470Z","shell.execute_reply.started":"2023-06-20T00:40:52.306214Z","shell.execute_reply":"2023-06-20T00:40:52.318540Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"def create_samples(model, preprocess, img_path, question, device, rotate=True):\n    samples = []\n    img = Image.open(img_path)\n    \n    encoded_q = model.encode_text(clip.tokenize(question).to(device))\n    encoded_img = model.encode_image(preprocess(img).unsqueeze(0).to(device))\n    samples.append(torch.hstack([encoded_img, encoded_q]))\n    if not rotate:\n        return torch.vstack(samples)\n\n    # Rotate Image By 90 Degree\n    rotated_image1 = img.rotate(90)\n    encoded_img = model.encode_image(preprocess(rotated_image1).unsqueeze(0).to(device))\n    samples.append(torch.hstack([encoded_img, encoded_q]))\n\n    # Rotate Image By 180 Degree\n    rotated_image2 = img.rotate(180)\n    encoded_img = model.encode_image(preprocess(rotated_image2).unsqueeze(0).to(device))\n    samples.append(torch.hstack([encoded_img, encoded_q]))\n\n    # Rotate Image By 270 Degree\n    rotated_image3 = img.rotate(270)\n    encoded_img = model.encode_image(preprocess(rotated_image3).unsqueeze(0).to(device))\n    samples.append(torch.hstack([encoded_img, encoded_q]))\n        \n    return torch.vstack(samples)\n        ","metadata":{"execution":{"iopub.status.busy":"2023-06-20T00:41:21.679846Z","iopub.execute_input":"2023-06-20T00:41:21.680223Z","iopub.status.idle":"2023-06-20T00:41:21.689500Z","shell.execute_reply.started":"2023-06-20T00:41:21.680193Z","shell.execute_reply":"2023-06-20T00:41:21.688407Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"X = []\n\nwith torch.no_grad():\n  for i, entry in enumerate(df):\n    X.extend(create_samples(model, preprocess, '/kaggle/input/vizwiz/data/Images/'+entry[0], entry[2], device))\n    if i%1000==0:\n        print('Checkpoint at: ' + str(i))\n\nX = torch.vstack(X)","metadata":{"id":"Z1MaUGvdOt0J","execution":{"iopub.status.busy":"2023-06-19T11:11:57.467506Z","iopub.execute_input":"2023-06-19T11:11:57.467884Z","iopub.status.idle":"2023-06-19T12:07:16.231770Z","shell.execute_reply.started":"2023-06-19T11:11:57.467836Z","shell.execute_reply":"2023-06-19T12:07:16.230733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(X,'/kaggle/working/X.pt')","metadata":{"execution":{"iopub.status.busy":"2023-06-19T12:11:47.605737Z","iopub.execute_input":"2023-06-19T12:11:47.606167Z","iopub.status.idle":"2023-06-19T12:11:47.948057Z","shell.execute_reply.started":"2023-06-19T12:11:47.606139Z","shell.execute_reply":"2023-06-19T12:11:47.946771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = torch.load('/kaggle/input/dfasdasd/X.pt')","metadata":{"execution":{"iopub.status.busy":"2023-06-19T23:24:35.186008Z","iopub.execute_input":"2023-06-19T23:24:35.186265Z","iopub.status.idle":"2023-06-19T23:24:36.226407Z","shell.execute_reply.started":"2023-06-19T23:24:35.186242Z","shell.execute_reply":"2023-06-19T23:24:36.225055Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"X_val = []\n\nwith torch.no_grad():\n  for i, entry in enumerate(val_df):\n    X_val.extend(create_samples(model, preprocess, '/kaggle/input/vizwiz/data/Images/'+entry[0], entry[2], device, rotate=False))\n    if i%1000==0:\n        print('Checkpoint at: ' + str(i))\n\nX_val = torch.vstack(X_val)","metadata":{"execution":{"iopub.status.busy":"2023-06-20T00:41:24.824667Z","iopub.execute_input":"2023-06-20T00:41:24.825368Z","iopub.status.idle":"2023-06-20T00:44:03.525095Z","shell.execute_reply.started":"2023-06-20T00:41:24.825333Z","shell.execute_reply":"2023-06-20T00:44:03.523966Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":"Checkpoint at: 0\nCheckpoint at: 1000\nCheckpoint at: 2000\nCheckpoint at: 3000\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.save(X_val,'/kaggle/working/X_val.pt')","metadata":{"execution":{"iopub.status.busy":"2023-06-20T00:46:14.530706Z","iopub.execute_input":"2023-06-20T00:46:14.531137Z","iopub.status.idle":"2023-06-20T00:46:14.554277Z","shell.execute_reply.started":"2023-06-20T00:46:14.531104Z","shell.execute_reply":"2023-06-20T00:46:14.553406Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"X_val = torch.load('/kaggle/input/dfasdasd/X_val.pt')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lb_types = preprocessing.LabelBinarizer()\n\nencoded_answer_types = lb_types.fit_transform(answer_types)\n\nprint(encoded_answer_types[10])","metadata":{"id":"jWct5pBRmMJo","outputId":"130ce37e-2519-46f4-f772-1e1db012df46","execution":{"iopub.status.busy":"2023-06-19T23:24:36.230859Z","iopub.execute_input":"2023-06-19T23:24:36.231211Z","iopub.status.idle":"2023-06-19T23:24:36.277615Z","shell.execute_reply.started":"2023-06-19T23:24:36.231180Z","shell.execute_reply":"2023-06-19T23:24:36.276823Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"[0 1 0 0]\n","output_type":"stream"}]},{"cell_type":"code","source":"lb_types.classes_","metadata":{"id":"MCLsMAE313sT","outputId":"c1614be4-f8e4-408a-a251-ff5268f7ae58","execution":{"iopub.status.busy":"2023-06-19T23:24:36.283208Z","iopub.execute_input":"2023-06-19T23:24:36.285847Z","iopub.status.idle":"2023-06-19T23:24:36.293991Z","shell.execute_reply.started":"2023-06-19T23:24:36.285812Z","shell.execute_reply":"2023-06-19T23:24:36.292785Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"array(['number', 'other', 'unanswerable', 'yes/no'], dtype='<U12')"},"metadata":{}}]},{"cell_type":"code","source":"lb_answers = preprocessing.LabelBinarizer()\n\nencoded_answers = lb_answers.fit_transform(answers)\n\nprint(encoded_answers[10])","metadata":{"execution":{"iopub.status.busy":"2023-06-19T23:24:36.295478Z","iopub.execute_input":"2023-06-19T23:24:36.295877Z","iopub.status.idle":"2023-06-19T23:24:36.462437Z","shell.execute_reply.started":"2023-06-19T23:24:36.295833Z","shell.execute_reply":"2023-06-19T23:24:36.461366Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"[0 0 0 ... 0 0 0]\n","output_type":"stream"}]},{"cell_type":"code","source":"lb_answers.classes_.shape","metadata":{"execution":{"iopub.status.busy":"2023-06-19T23:32:06.488483Z","iopub.execute_input":"2023-06-19T23:32:06.488870Z","iopub.status.idle":"2023-06-19T23:32:06.495713Z","shell.execute_reply.started":"2023-06-19T23:32:06.488838Z","shell.execute_reply":"2023-06-19T23:32:06.494688Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"(11778,)"},"metadata":{}}]},{"cell_type":"code","source":"y = [(a_type, a) for a_type, a in zip(encoded_answer_types, encoded_answers)]","metadata":{"id":"vfQLzWjgmntz","execution":{"iopub.status.busy":"2023-06-19T23:24:36.464036Z","iopub.execute_input":"2023-06-19T23:24:36.465119Z","iopub.status.idle":"2023-06-19T23:24:36.483518Z","shell.execute_reply.started":"2023-06-19T23:24:36.465087Z","shell.execute_reply":"2023-06-19T23:24:36.482673Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"tmp_y = y\ny = []\nfor t in tmp_y:\n    for i in range(4):\n        y.append(t)","metadata":{"execution":{"iopub.status.busy":"2023-06-19T23:30:20.749176Z","iopub.execute_input":"2023-06-19T23:30:20.749540Z","iopub.status.idle":"2023-06-19T23:30:20.773313Z","shell.execute_reply.started":"2023-06-19T23:30:20.749510Z","shell.execute_reply":"2023-06-19T23:30:20.772422Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"val_encoded_answer_types = lb_types.transform(val_answer_types)\nval_encoded_answers = lb_answers.transform(val_answers)\ny_val = [(a_type, a) for a_type, a in zip(val_encoded_answer_types, val_encoded_answers)]","metadata":{"execution":{"iopub.status.busy":"2023-06-20T00:47:19.476737Z","iopub.execute_input":"2023-06-20T00:47:19.477886Z","iopub.status.idle":"2023-06-20T00:47:19.522276Z","shell.execute_reply.started":"2023-06-20T00:47:19.477827Z","shell.execute_reply":"2023-06-20T00:47:19.521282Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"markdown","source":"# This is a problem :)","metadata":{}},{"cell_type":"code","source":"c = 0\nfor ans in val_encoded_answers:\n    c += ans.sum()\nprint(c)\nprint(len(val_encoded_answers))","metadata":{"execution":{"iopub.status.busy":"2023-06-20T01:08:25.683710Z","iopub.execute_input":"2023-06-20T01:08:25.684581Z","iopub.status.idle":"2023-06-20T01:08:25.711394Z","shell.execute_reply.started":"2023-06-20T01:08:25.684538Z","shell.execute_reply":"2023-06-20T01:08:25.710471Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stdout","text":"1553\n3173\n","output_type":"stream"}]},{"cell_type":"code","source":"c = 0\nfor ans in val_encoded_answer_types:\n    c += ans.sum()\nprint(c)\nprint(len(val_encoded_answer_types))","metadata":{"execution":{"iopub.status.busy":"2023-06-20T01:13:13.968834Z","iopub.execute_input":"2023-06-20T01:13:13.969597Z","iopub.status.idle":"2023-06-20T01:13:13.988296Z","shell.execute_reply.started":"2023-06-20T01:13:13.969562Z","shell.execute_reply":"2023-06-20T01:13:13.987340Z"},"trusted":true},"execution_count":74,"outputs":[{"name":"stdout","text":"3173\n3173\n","output_type":"stream"}]},{"cell_type":"code","source":"X = X.cpu().detach().numpy()","metadata":{"execution":{"iopub.status.busy":"2023-06-19T23:27:45.468204Z","iopub.execute_input":"2023-06-19T23:27:45.468569Z","iopub.status.idle":"2023-06-19T23:27:45.606427Z","shell.execute_reply.started":"2023-06-19T23:27:45.468540Z","shell.execute_reply":"2023-06-19T23:27:45.605538Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=42)","metadata":{"id":"sTOVgxoWc5d7","execution":{"iopub.status.busy":"2023-06-19T23:31:40.669270Z","iopub.execute_input":"2023-06-19T23:31:40.669645Z","iopub.status.idle":"2023-06-19T23:31:40.757833Z","shell.execute_reply.started":"2023-06-19T23:31:40.669611Z","shell.execute_reply":"2023-06-19T23:31:40.756828Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\n\n# Custom Dataset class\nclass VQADataset(Dataset):\n    def __init__(self, data, labels):\n        self.data = data\n        self.atypes = []\n        self.answers = []\n        for atype, ans in labels:\n          self.atypes.append(atype)\n          self.answers.append(ans)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        x = torch.Tensor(self.data[index])\n        at, a = torch.Tensor(self.atypes[index]), torch.Tensor(self.answers[index])\n\n        return x, at, a","metadata":{"id":"tBbAMFNud_kM","execution":{"iopub.status.busy":"2023-06-19T23:31:42.558725Z","iopub.execute_input":"2023-06-19T23:31:42.559414Z","iopub.status.idle":"2023-06-19T23:31:42.566862Z","shell.execute_reply.started":"2023-06-19T23:31:42.559381Z","shell.execute_reply":"2023-06-19T23:31:42.565980Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"train_set = DataLoader(VQADataset(X_train, y_train), batch_size=512)\ntest_set = DataLoader(VQADataset(X_test, y_test), batch_size=512)\nval_set = DataLoader(VQADataset(X_val, y_val), batch_size=512)","metadata":{"id":"GJAc2XTIeMou","execution":{"iopub.status.busy":"2023-06-20T01:04:17.242232Z","iopub.execute_input":"2023-06-20T01:04:17.242599Z","iopub.status.idle":"2023-06-20T01:04:17.277640Z","shell.execute_reply.started":"2023-06-20T01:04:17.242570Z","shell.execute_reply":"2023-06-20T01:04:17.276689Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"class VQA_Network(nn.Module):\n    def __init__(self, num_classes, vocab_size, hidden_dim, embedding_dim):\n        super(VQA_Network, self).__init__()\n\n        self.fc = nn.Linear(embedding_dim, hidden_dim)\n        self.fc2_answers = nn.Linear(hidden_dim, vocab_size)\n\n        self.fc2_aux = nn.Linear(hidden_dim, num_classes)\n        self.fc3_aux = nn.Linear(num_classes, vocab_size)\n\n        self.norm = nn.LayerNorm(hidden_dim)\n        self.dropout = nn.Dropout()\n        self.softmax = nn.Softmax(dim=0)\n\n    def forward(self, x):\n        x = x.to(dtype=torch.float32)\n        x = self.fc(x)\n        x = self.norm(x)\n        x = self.dropout(x)\n\n        atype = self.softmax(self.fc2_aux(x))\n        aux = self.fc3_aux(atype)\n\n        answers = self.fc2_answers(x)\n\n        answers = answers * aux\n\n        # answers = answers.to(dtype=torch.int64)\n\n        return self.softmax(answers), atype\n","metadata":{"id":"w0LAOTWvjh9w","execution":{"iopub.status.busy":"2023-06-20T00:47:29.320711Z","iopub.execute_input":"2023-06-20T00:47:29.321100Z","iopub.status.idle":"2023-06-20T00:47:29.333222Z","shell.execute_reply.started":"2023-06-20T00:47:29.321067Z","shell.execute_reply":"2023-06-20T00:47:29.332185Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"num_classes = 4\nvocab_size = 11778\nhidden_dim = 512\nembedding_dim = 1024\n\nnum_epochs = 200\n\n# Instantiate the model\nmodel = VQA_Network(num_classes, vocab_size, hidden_dim, embedding_dim).to(device)\n\n# Define loss function\ncriterion = nn.CrossEntropyLoss()\n\n# Define optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n\n# Training loop\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss_a = 0.0\n    running_loss_at = 0.0\n    acc_a = 0\n    acc_at = 0\n    val_running_loss_a = 0.0\n    val_running_loss_at = 0.0\n    val_acc_a = 0\n    val_acc_at = 0\n\n    for xs, atypeset, answerset in train_set:\n        # Move data to the device\n        xs = xs.to(device)\n        atypeset = atypeset.to(device)\n        answerset = answerset.to(device)\n\n        # Clear gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        answers, atypes = model(xs)\n        # print(xs.size())\n        # print(labelset.size())\n        # Compute loss\n        answer_loss = criterion(answers, answerset)\n\n        # Backward pass\n        answer_loss.backward(retain_graph=True)\n\n        # Compute loss\n        atype_loss = criterion(atypes, atypeset)\n\n        # Backward pass\n        atype_loss.backward()\n\n        # Update weights\n        optimizer.step()\n\n        # Update running loss\n        running_loss_a += answer_loss.item()\n        running_loss_at += atype_loss.item()\n        acc_at += (torch.argmax(atypes, 1) == torch.argmax(atypeset, 1)).float().sum()\n        acc_a += (torch.argmax(answers, 1) == torch.argmax(answerset, 1)).float().sum()\n\n    acc_a = acc_a / len(y_train)\n    acc_at = acc_at / len(y_train)\n    # Calculate average loss for the epoch\n    average_loss_a = running_loss_a / len(y_train)\n    average_loss_at = running_loss_at / len(y_train)\n\n    # Print progress\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss_a: {average_loss_a:.4f}, Loss_at: {average_loss_at:.4f}\", end=' ')\n    print(f\"Acc_a: {acc_a:.4f}, Acc_at: {acc_at:.4f}\", end='\\t')\n    \n    with torch.no_grad():\n        model.eval()\n        for xs, atypeset, answerset in val_set:\n            # Move data to the device\n            xs = xs.to(device)\n            atypeset = atypeset.to(device)\n            answerset = answerset.to(device)\n\n            # Forward pass\n            answers, atypes = model(xs)\n            # print(xs.size())\n            # print(labelset.size())\n            # Compute loss\n            answer_loss = criterion(answers, answerset)\n\n            # Compute loss\n            atype_loss = criterion(atypes, atypeset)\n\n            # Update running loss\n            val_running_loss_a += answer_loss.item()\n            val_running_loss_at += atype_loss.item()\n            val_acc_at += (torch.argmax(atypes, 1) == torch.argmax(atypeset, 1)).float().sum()\n            val_acc_a += (torch.argmax(answers, 1) == torch.argmax(answerset, 1)).float().sum()\n\n        val_acc_a = val_acc_a / len(y_val)\n        val_acc_at = val_acc_at / len(y_val)\n        # Calculate average loss for the epoch\n        val_average_loss_a = val_running_loss_a / len(y_val)\n        val_average_loss_at = val_running_loss_at / len(y_val)\n\n        # Print progress\n        print(f\"Validation: Loss_a: {val_average_loss_a:.4f}, Loss_at: {val_average_loss_at:.4f}\", end=' ')\n        print(f\"Acc_a: {val_acc_a:.4f}, Acc_at: {val_acc_at:.4f}\")\n\n\n# Training complete\n","metadata":{"id":"V_K1-Pgwe4pg","execution":{"iopub.status.busy":"2023-06-20T01:04:26.053368Z","iopub.execute_input":"2023-06-20T01:04:26.053728Z","iopub.status.idle":"2023-06-20T01:07:28.171395Z","shell.execute_reply.started":"2023-06-20T01:04:26.053696Z","shell.execute_reply":"2023-06-20T01:07:28.169855Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"Epoch [1/200], Loss_a: 0.0184, Loss_at: 0.0027 Acc_a: 0.0088, Acc_at: 0.5099\tValidation: Loss_a: 0.0102, Loss_at: 0.0030 Acc_a: 0.0552, Acc_at: 0.5979\nEpoch [2/200], Loss_a: 0.0184, Loss_at: 0.0027 Acc_a: 0.0419, Acc_at: 0.5653\tValidation: Loss_a: 0.0102, Loss_at: 0.0030 Acc_a: 0.0637, Acc_at: 0.5830\nEpoch [3/200], Loss_a: 0.0184, Loss_at: 0.0027 Acc_a: 0.0498, Acc_at: 0.5694\tValidation: Loss_a: 0.0101, Loss_at: 0.0030 Acc_a: 0.0561, Acc_at: 0.6061\nEpoch [4/200], Loss_a: 0.0184, Loss_at: 0.0027 Acc_a: 0.0590, Acc_at: 0.5708\tValidation: Loss_a: 0.0101, Loss_at: 0.0030 Acc_a: 0.0539, Acc_at: 0.6120\nEpoch [5/200], Loss_a: 0.0184, Loss_at: 0.0027 Acc_a: 0.0754, Acc_at: 0.5806\tValidation: Loss_a: 0.0101, Loss_at: 0.0030 Acc_a: 0.0596, Acc_at: 0.5846\nEpoch [6/200], Loss_a: 0.0183, Loss_at: 0.0027 Acc_a: 0.0984, Acc_at: 0.5566\tValidation: Loss_a: 0.0101, Loss_at: 0.0030 Acc_a: 0.0608, Acc_at: 0.5736\nEpoch [7/200], Loss_a: 0.0183, Loss_at: 0.0027 Acc_a: 0.1280, Acc_at: 0.5501\tValidation: Loss_a: 0.0101, Loss_at: 0.0030 Acc_a: 0.0592, Acc_at: 0.5742\nEpoch [8/200], Loss_a: 0.0183, Loss_at: 0.0027 Acc_a: 0.1583, Acc_at: 0.5373\tValidation: Loss_a: 0.0101, Loss_at: 0.0030 Acc_a: 0.0599, Acc_at: 0.5689\nEpoch [9/200], Loss_a: 0.0183, Loss_at: 0.0027 Acc_a: 0.1920, Acc_at: 0.5496\tValidation: Loss_a: 0.0101, Loss_at: 0.0030 Acc_a: 0.0586, Acc_at: 0.6026\nEpoch [10/200], Loss_a: 0.0182, Loss_at: 0.0027 Acc_a: 0.2269, Acc_at: 0.5666\tValidation: Loss_a: 0.0101, Loss_at: 0.0030 Acc_a: 0.0599, Acc_at: 0.5890\nEpoch [11/200], Loss_a: 0.0182, Loss_at: 0.0027 Acc_a: 0.2620, Acc_at: 0.5701\tValidation: Loss_a: 0.0101, Loss_at: 0.0030 Acc_a: 0.0611, Acc_at: 0.6237\nEpoch [12/200], Loss_a: 0.0181, Loss_at: 0.0027 Acc_a: 0.2946, Acc_at: 0.5907\tValidation: Loss_a: 0.0101, Loss_at: 0.0030 Acc_a: 0.0599, Acc_at: 0.6366\nEpoch [13/200], Loss_a: 0.0180, Loss_at: 0.0027 Acc_a: 0.3285, Acc_at: 0.6002\tValidation: Loss_a: 0.0100, Loss_at: 0.0030 Acc_a: 0.0602, Acc_at: 0.6401\nEpoch [14/200], Loss_a: 0.0180, Loss_at: 0.0027 Acc_a: 0.3596, Acc_at: 0.6057\tValidation: Loss_a: 0.0100, Loss_at: 0.0030 Acc_a: 0.0589, Acc_at: 0.6373\nEpoch [15/200], Loss_a: 0.0179, Loss_at: 0.0027 Acc_a: 0.3899, Acc_at: 0.6167\tValidation: Loss_a: 0.0100, Loss_at: 0.0030 Acc_a: 0.0574, Acc_at: 0.6495\nEpoch [16/200], Loss_a: 0.0178, Loss_at: 0.0027 Acc_a: 0.4186, Acc_at: 0.6254\tValidation: Loss_a: 0.0100, Loss_at: 0.0030 Acc_a: 0.0586, Acc_at: 0.6640\nEpoch [17/200], Loss_a: 0.0177, Loss_at: 0.0027 Acc_a: 0.4459, Acc_at: 0.6282\tValidation: Loss_a: 0.0100, Loss_at: 0.0030 Acc_a: 0.0602, Acc_at: 0.6754\nEpoch [18/200], Loss_a: 0.0176, Loss_at: 0.0027 Acc_a: 0.4699, Acc_at: 0.6328\tValidation: Loss_a: 0.0100, Loss_at: 0.0030 Acc_a: 0.0592, Acc_at: 0.6789\nEpoch [19/200], Loss_a: 0.0176, Loss_at: 0.0027 Acc_a: 0.4914, Acc_at: 0.6293\tValidation: Loss_a: 0.0100, Loss_at: 0.0030 Acc_a: 0.0592, Acc_at: 0.6880\nEpoch [20/200], Loss_a: 0.0175, Loss_at: 0.0027 Acc_a: 0.5140, Acc_at: 0.6371\tValidation: Loss_a: 0.0100, Loss_at: 0.0030 Acc_a: 0.0586, Acc_at: 0.6697\nEpoch [21/200], Loss_a: 0.0175, Loss_at: 0.0027 Acc_a: 0.5330, Acc_at: 0.6352\tValidation: Loss_a: 0.0100, Loss_at: 0.0030 Acc_a: 0.0574, Acc_at: 0.6807\nEpoch [22/200], Loss_a: 0.0174, Loss_at: 0.0027 Acc_a: 0.5502, Acc_at: 0.6340\tValidation: Loss_a: 0.0100, Loss_at: 0.0030 Acc_a: 0.0577, Acc_at: 0.6716\nEpoch [23/200], Loss_a: 0.0174, Loss_at: 0.0027 Acc_a: 0.5668, Acc_at: 0.6335\tValidation: Loss_a: 0.0100, Loss_at: 0.0030 Acc_a: 0.0577, Acc_at: 0.6880\nEpoch [24/200], Loss_a: 0.0173, Loss_at: 0.0027 Acc_a: 0.5822, Acc_at: 0.6373\tValidation: Loss_a: 0.0100, Loss_at: 0.0030 Acc_a: 0.0570, Acc_at: 0.6814\nEpoch [25/200], Loss_a: 0.0173, Loss_at: 0.0027 Acc_a: 0.5958, Acc_at: 0.6394\tValidation: Loss_a: 0.0100, Loss_at: 0.0030 Acc_a: 0.0577, Acc_at: 0.6877\nEpoch [26/200], Loss_a: 0.0172, Loss_at: 0.0027 Acc_a: 0.6097, Acc_at: 0.6434\tValidation: Loss_a: 0.0100, Loss_at: 0.0030 Acc_a: 0.0580, Acc_at: 0.6760\nEpoch [27/200], Loss_a: 0.0172, Loss_at: 0.0027 Acc_a: 0.6220, Acc_at: 0.6459\tValidation: Loss_a: 0.0100, Loss_at: 0.0030 Acc_a: 0.0564, Acc_at: 0.6798\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[72], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m val_acc_a \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     27\u001b[0m val_acc_at \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m xs, atypeset, answerset \u001b[38;5;129;01min\u001b[39;00m train_set:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Move data to the device\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     xs \u001b[38;5;241m=\u001b[39m xs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     32\u001b[0m     atypeset \u001b[38;5;241m=\u001b[39m atypeset\u001b[38;5;241m.\u001b[39mto(device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[0;32mIn[39], line 18\u001b[0m, in \u001b[0;36mVQADataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[1;32m     17\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[index])\n\u001b[0;32m---> 18\u001b[0m     at, a \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matypes[index]), \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manswers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x, at, a\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"y_test[3]","metadata":{"id":"Mp5kfukWCdV4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model(X_test[3]))","metadata":{"id":"8kYm-m3tCes3"},"execution_count":null,"outputs":[]}]}